{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6422b51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m      3\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Vectors\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import col, countDistinct, isnan, when, sum\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark_home = \"C:\\\\Program Files\\\\ApacheSpark\"\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "\n",
    "# Add Spark bin and executors to PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"bin\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"sbin\")\n",
    "\n",
    "# Add Spark Python libraries to PYTHONPATH\n",
    "os.environ[\"PYTHONPATH\"] = os.path.join(spark_home, \"python\") + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PYTHONPATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\")\n",
    "\n",
    "# Add PySpark to the system path\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"pyspark.zip\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9-src.zip\")\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b15f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailInsight-Optimizer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6ce84",
   "metadata": {},
   "source": [
    "# Load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40b2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../data/data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34197f8",
   "metadata": {},
   "source": [
    "# Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c380339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  3900\n",
      "Sample data: \n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|Customer ID|Age|Gender|Item Purchased|Category|Purchase Amount (USD)|     Location|Size|    Color|Season|Review Rating|Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|          1| 55|  Male|        Blouse|Clothing|                   53|     Kentucky|   L|     Gray|Winter|          3.1|                Yes|      Express|             Yes|            Yes|                14|         Venmo|           Fortnightly|\n",
      "|          2| 19|  Male|       Sweater|Clothing|                   64|        Maine|   L|   Maroon|Winter|          3.1|                Yes|      Express|             Yes|            Yes|                 2|          Cash|           Fortnightly|\n",
      "|          3| 50|  Male|         Jeans|Clothing|                   73|Massachusetts|   S|   Maroon|Spring|          3.1|                Yes|Free Shipping|             Yes|            Yes|                23|   Credit Card|                Weekly|\n",
      "|          4| 21|  Male|       Sandals|Footwear|                   90| Rhode Island|   M|   Maroon|Spring|          3.5|                Yes| Next Day Air|             Yes|            Yes|                49|        PayPal|                Weekly|\n",
      "|          5| 45|  Male|        Blouse|Clothing|                   49|       Oregon|   M|Turquoise|Spring|          2.7|                Yes|Free Shipping|             Yes|            Yes|                31|        PayPal|              Annually|\n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records: \", data.rdd.count())\n",
    "\n",
    "print(\"Sample data: \")\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e62372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data schema: \n",
      "root\n",
      " |-- Customer ID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Item Purchased: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Purchase Amount (USD): integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Review Rating: double (nullable = true)\n",
      " |-- Subscription Status: string (nullable = true)\n",
      " |-- Shipping Type: string (nullable = true)\n",
      " |-- Discount Applied: string (nullable = true)\n",
      " |-- Promo Code Used: string (nullable = true)\n",
      " |-- Previous Purchases: integer (nullable = true)\n",
      " |-- Payment Method: string (nullable = true)\n",
      " |-- Frequency of Purchases: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Data schema: \")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01317b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary: \n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|summary|       Customer ID|              Age|Gender|Item Purchased|   Category|Purchase Amount (USD)|Location|Size| Color|Season|     Review Rating|Subscription Status| Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|  count|              3900|             3900|  3900|          3900|       3900|                 3900|    3900|3900|  3900|  3900|              3900|               3900|          3900|            3900|           3900|              3900|          3900|                  3900|\n",
      "|   mean|            1950.5|44.06846153846154|  NULL|          NULL|       NULL|    59.76435897435898|    NULL|NULL|  NULL|  NULL| 3.749948717948712|               NULL|          NULL|            NULL|           NULL| 25.35153846153846|          NULL|                  NULL|\n",
      "| stddev|1125.9773532358456|15.20758912716237|  NULL|          NULL|       NULL|   23.685392250875328|    NULL|NULL|  NULL|  NULL|0.7162228139312412|               NULL|          NULL|            NULL|           NULL|14.447125170462305|          NULL|                  NULL|\n",
      "|    min|                 1|               18|Female|      Backpack|Accessories|                   20| Alabama|   L| Beige|  Fall|               2.5|                 No|2-Day Shipping|              No|             No|                 1| Bank Transfer|              Annually|\n",
      "|    max|              3900|               70|  Male|       T-shirt|  Outerwear|                  100| Wyoming|  XL|Yellow|Winter|               5.0|                Yes|  Store Pickup|             Yes|            Yes|                50|         Venmo|                Weekly|\n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data summary: \")\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c6b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values: \n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "|Customer ID_unique|Age_unique|Gender_unique|Item Purchased_unique|Category_unique|Purchase Amount (USD)_unique|Location_unique|Size_unique|Color_unique|Season_unique|Review Rating_unique|Subscription Status_unique|Shipping Type_unique|Discount Applied_unique|Promo Code Used_unique|Previous Purchases_unique|Payment Method_unique|Frequency of Purchases_unique|\n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "|              3900|        53|            2|                   25|              4|                          81|             50|          4|          25|            4|                  26|                         2|                   6|                      2|                     2|                       50|                    6|                            7|\n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Number of unique values: \")\n",
    "data.select([countDistinct(col).alias(f\"{col}_unique\") for col in data.columns]).show()\n",
    "\n",
    "# for column in data.columns:\n",
    "#     print(f\"Column: {column}\")\n",
    "#     data.select(column).distinct().show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196cb79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for nulls: \n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|Customer ID|Age|Gender|Item Purchased|Category|Purchase Amount (USD)|Location|Size|Color|Season|Review Rating|Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|          0|  0|     0|             0|       0|                    0|       0|   0|    0|     0|            0|                  0|            0|               0|              0|                 0|             0|                     0|\n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Check for nulls: \")\n",
    "null_nan_counts = data.select([\n",
    "    sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)).alias(c)\n",
    "    for c in data.columns\n",
    "])\n",
    "null_nan_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedc018",
   "metadata": {},
   "source": [
    "As we inspecting the data, we found that it's mainly about apparel products. It has good analytical features as,\n",
    "- No nulls\n",
    "- No outliers as seen in **Min** and **Max** of each column in the summary\n",
    "- Standardized format of values (e.g. Gender has 2 values (Male, Female) Which always appear in consistent formats, no M or F for example)\n",
    "\n",
    "However some issues were detected\n",
    "- Checking the Customer ID column, it doesn't have any useful indication, so it can be dropped. \n",
    "- Categorical data should be encoded for better machine learning model training (e.g., Yes/No to 1/0).\n",
    "- Numerical data should be standardized to avoid bias to certain features.\n",
    "- Binning columns as age into categories (e.g., '18-25', '26-35', etc.) can capture non-linear relationships and make the model more interpretable. The same applies for Purchase amount which can be Purchase Tier: (e.g., 'Low', 'Medium', 'High') since. We might then drop the original columns.\n",
    "- Finally, splitting the data for model train and test datasets should be done (Data is split first then standardized based on training set statistics to avoid data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d339e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns of irrelevant data\n",
    "data = data.drop(\"Customer ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe3567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning columns\n",
    "import preprocessing_utils as pf\n",
    "\n",
    "def bin_all_columns(row):\n",
    "    try:\n",
    "        new_age = pf.bin_age(row)\n",
    "        new_purchase_amount = pf.bin_purchase_amount(row)\n",
    "        new_previous_purchase = pf.bin_previous_purchases(row)\n",
    "\n",
    "        new_row = row.asDict()\n",
    "        new_row[\"Age\"] = new_age\n",
    "        new_row[\"Purchase Amount (USD)\"] = new_purchase_amount\n",
    "        new_row[\"Previous Purchases\"] = new_previous_purchase\n",
    "\n",
    "        return Row(**new_row)\n",
    "    except Exception as e:\n",
    "        print(\"Error processing row:\", row)\n",
    "        print(\"Error message:\", e)\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "rdd = data.rdd\n",
    "rdd = rdd.map(bin_all_columns)\n",
    "data = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1821fc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+\n",
      "|Review Rating|Age         |Gender|Item Purchased                                                             |Category    |Purchase Amount (USD)|Location                                                                                                                                              |Size        |Color                                                                      |Season      |Subscription Status|Shipping Type     |Discount Applied|Promo Code Used|Previous Purchases            |Payment Method    |Frequency of Purchases|\n",
      "+-------------+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+\n",
      "|3.1          |[1, 0, 0, 0]|[1, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0]             |[1, 0, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0, 0] |\n",
      "|3.1          |[0, 1, 0, 0]|[1, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0]             |[1, 0, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0, 0] |\n",
      "|3.1          |[0, 0, 1, 0]|[1, 0]|[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[1, 0]             |[0, 1, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0, 0, 0]|[0, 1, 0, 0, 0, 0, 0] |\n",
      "|3.5          |[0, 1, 0, 0]|[1, 0]|[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[0, 1, 0]            |[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[1, 0]             |[0, 0, 1, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 1, 0, 0, 0, 0, 0] |\n",
      "|2.7          |[0, 0, 1, 0]|[1, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[1, 0]             |[0, 1, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 0, 1, 0, 0, 0, 0] |\n",
      "+-------------+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Categorical columns (all but not 'Review Rating')\n",
    "categorical_cols = data.columns\n",
    "categorical_cols.remove(\"Review Rating\")\n",
    "\n",
    "distinct_values = {}\n",
    "for col in categorical_cols:\n",
    "    distinct_values[col] = rdd.map(lambda row: row[col]).distinct().collect()\n",
    "\n",
    "# Index mappings for each categorical column\n",
    "category_to_index = {}\n",
    "for col in categorical_cols:\n",
    "    category_to_index[col] = {value: idx for idx, value in enumerate(distinct_values[col])}\n",
    "\n",
    "# Applying one-hot encoding and string indexing\n",
    "def encode_row(row):\n",
    "    # Convert Row to dictionary so we can modify it\n",
    "    row_dict = row.asDict()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        index = category_to_index[col].get(row[col], -1) \n",
    "        row_dict[col + \"_index\"] = index\n",
    "        \n",
    "        one_hot = [0] * len(distinct_values[col])\n",
    "        if index >= 0:\n",
    "            one_hot[index] = 1  # Set the corresponding index to 1\n",
    "        row_dict[col + \"_onehot\"] = one_hot\n",
    "    \n",
    "    return Row(**row_dict)\n",
    "\n",
    "# Apply the encoding logic with Map\n",
    "encoded_rdd = rdd.map(encode_row)\n",
    "encoded_data = spark.createDataFrame(encoded_rdd)\n",
    "\n",
    "encoded_data = encoded_data.drop(*([col + \"_index\" for col in categorical_cols] + [col for col in categorical_cols]))\n",
    "\n",
    "for col in encoded_data.columns:\n",
    "    if \"_onehot\" in col:\n",
    "        new_col = col.replace(\"_onehot\", \"\")\n",
    "        encoded_data = encoded_data.withColumnRenamed(col, new_col)\n",
    "\n",
    "# Show the result\n",
    "encoded_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e4378bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count:  3177\n",
      "Test data count:  723\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "|Age         |Gender|Item Purchased                                                             |Category    |Purchase Amount (USD)|Location                                                                                                                                              |Size        |Color                                                                      |Season      |Subscription Status|Shipping Type     |Discount Applied|Promo Code Used|Previous Purchases            |Payment Method    |Frequency of Purchases|Review Rating (scaled)|\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[0, 0, 0, 1]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1]             |[1, 0, 0, 0, 0, 0]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 0, 0, 0, 1, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]|[0, 0, 0, 1]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[0, 1, 0, 0]|[0, 1]             |[0, 0, 0, 0, 1, 0]|[0, 1]          |[0, 1]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 0, 1, 0]|[0, 0, 0, 0, 1, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[1, 0, 0, 0]|[0, 1]             |[0, 0, 0, 0, 0, 1]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]|[0, 0, 1, 0, 0, 0]|[0, 0, 0, 0, 0, 0, 1] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 1]             |[0, 0, 0, 1, 0, 0]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[0, 0, 0, 0, 0, 1]|[0, 0, 0, 0, 1, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]|[0, 1, 0, 0]|[0, 0, 1]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]|[0, 0, 1, 0]|[1, 0]             |[0, 0, 1, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 0, 0, 1]|[0, 1, 0, 0, 0, 0, 0] |-1.72554459347266     |\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardizing numerical columns\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Assemble the numerical column into a vector\n",
    "assembler = VectorAssembler(inputCols=[\"Review Rating\"], outputCol=\"review_vector\")\n",
    "assembled_data = assembler.transform(encoded_data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = assembled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Training data count: \", train_data.count())\n",
    "print(\"Test data count: \", test_data.count())\n",
    "\n",
    "# Apply standardization\n",
    "scaler = StandardScaler(inputCol=\"review_vector\", outputCol=\"review_scaled\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(train_data)\n",
    "scaled_data = scaler_model.transform(train_data)\n",
    "scaled_test = scaler_model.transform(test_data)\n",
    "\n",
    "# Drop original scalar column and rename scaled vector\n",
    "scaled_data = scaled_data.drop(\"Review Rating\", \"review_vector\").withColumnRenamed(\"review_scaled\", \"Review Rating (scaled)\")\n",
    "\n",
    "scaled_data = scaled_data.withColumn(\"Review Rating (scaled)\", vector_to_array(\"Review Rating (scaled)\")[0])\n",
    "scaled_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final datasets\n",
    "# test_data, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cell to add to preprocessing.ipynb\n",
    "\n",
    "# Save processed data for R segmentation analysis\n",
    "print(\"Saving preprocessed data for R segmentation analysis...\")\n",
    "\n",
    "# Define paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create ../data directory if it doesn't exist\n",
    "data_dir = Path(\"../data\")\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir(parents=True)\n",
    "\n",
    "# Convert Spark DataFrames to Pandas\n",
    "try:\n",
    "    train_pandas = scaled_data.toPandas()\n",
    "    test_pandas = scaled_test.toPandas()\n",
    "    \n",
    "    # Handle complex data types (like sparse vectors)\n",
    "    for col in train_pandas.columns:\n",
    "        if train_pandas[col].dtype == object:\n",
    "            train_pandas[col] = train_pandas[col].astype(str)\n",
    "            \n",
    "    for col in test_pandas.columns:\n",
    "        if test_pandas[col].dtype == object:\n",
    "            test_pandas[col] = test_pandas[col].astype(str)\n",
    "    \n",
    "    # Save to CSV files in the data directory\n",
    "    train_file = data_dir / \"data.csv\"\n",
    "    test_file = data_dir / \"test_data.csv\"\n",
    "    \n",
    "    train_pandas.to_csv(train_file, index=False)\n",
    "    test_pandas.to_csv(test_file, index=False)\n",
    "    \n",
    "    print(f\"✓ Successfully saved {train_pandas.shape[0]} training records to {train_file}\")\n",
    "    print(f\"✓ Successfully saved {test_pandas.shape[0]} test records to {test_file}\")\n",
    "    \n",
    "    # Create a command to run the R script\n",
    "    feature_eng_dir = Path(\"../feature engineering\")\n",
    "    r_script = feature_eng_dir / \"main.R\"\n",
    "    \n",
    "    print(\"\\nTo run R segmentation analysis, use this command:\")\n",
    "    print(f\"cd '{feature_eng_dir.absolute()}' && Rscript main.R\")\n",
    "    \n",
    "    # Option to run R script directly\n",
    "    run_r = input(\"Run R segmentation now? (y/n): \")\n",
    "    if run_r.lower() == 'y':\n",
    "        import subprocess\n",
    "        \n",
    "        # Change to the feature engineering directory and run the script\n",
    "        cmd = f\"cd '{feature_eng_dir.absolute()}' && Rscript main.R\"\n",
    "        print(f\"\\nRunning: {cmd}\")\n",
    "        \n",
    "        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        \n",
    "        # Show output in real-time\n",
    "        print(\"\\nR script output:\")\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == '' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "                \n",
    "        # Get the return code\n",
    "        return_code = process.poll()\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(\"\\n✓ R segmentation completed successfully\")\n",
    "        else:\n",
    "            print(f\"\\n✗ R script failed with return code {return_code}\")\n",
    "            print(\"Error output:\")\n",
    "            print(process.stderr.read())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Failed to save data or run R script.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
