{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6422b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import col, countDistinct, isnan, when, sum\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark_home = \"C:\\\\Program Files\\\\ApacheSpark\"\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "\n",
    "# Add Spark bin and executors to PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"bin\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"sbin\")\n",
    "\n",
    "# Add Spark Python libraries to PYTHONPATH\n",
    "os.environ[\"PYTHONPATH\"] = os.path.join(spark_home, \"python\") + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PYTHONPATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\")\n",
    "\n",
    "# Add PySpark to the system path\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"pyspark.zip\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9-src.zip\")\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b15f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailInsight-Optimizer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6ce84",
   "metadata": {},
   "source": [
    "# Load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a40b2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../data/data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34197f8",
   "metadata": {},
   "source": [
    "# Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c380339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  3900\n",
      "Sample data: \n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|Customer ID|Age|Gender|Item Purchased|Category|Purchase Amount (USD)|     Location|Size|    Color|Season|Review Rating|Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|          1| 55|  Male|        Blouse|Clothing|                   53|     Kentucky|   L|     Gray|Winter|          3.1|                Yes|      Express|             Yes|            Yes|                14|         Venmo|           Fortnightly|\n",
      "|          2| 19|  Male|       Sweater|Clothing|                   64|        Maine|   L|   Maroon|Winter|          3.1|                Yes|      Express|             Yes|            Yes|                 2|          Cash|           Fortnightly|\n",
      "|          3| 50|  Male|         Jeans|Clothing|                   73|Massachusetts|   S|   Maroon|Spring|          3.1|                Yes|Free Shipping|             Yes|            Yes|                23|   Credit Card|                Weekly|\n",
      "|          4| 21|  Male|       Sandals|Footwear|                   90| Rhode Island|   M|   Maroon|Spring|          3.5|                Yes| Next Day Air|             Yes|            Yes|                49|        PayPal|                Weekly|\n",
      "|          5| 45|  Male|        Blouse|Clothing|                   49|       Oregon|   M|Turquoise|Spring|          2.7|                Yes|Free Shipping|             Yes|            Yes|                31|        PayPal|              Annually|\n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records: \", data.rdd.count())\n",
    "\n",
    "print(\"Sample data: \")\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54e62372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data schema: \n",
      "root\n",
      " |-- Customer ID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Item Purchased: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Purchase Amount (USD): integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Review Rating: double (nullable = true)\n",
      " |-- Subscription Status: string (nullable = true)\n",
      " |-- Shipping Type: string (nullable = true)\n",
      " |-- Discount Applied: string (nullable = true)\n",
      " |-- Promo Code Used: string (nullable = true)\n",
      " |-- Previous Purchases: integer (nullable = true)\n",
      " |-- Payment Method: string (nullable = true)\n",
      " |-- Frequency of Purchases: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Data schema: \")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01317b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary: \n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|summary|       Customer ID|              Age|Gender|Item Purchased|   Category|Purchase Amount (USD)|Location|Size| Color|Season|     Review Rating|Subscription Status| Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|  count|              3900|             3900|  3900|          3900|       3900|                 3900|    3900|3900|  3900|  3900|              3900|               3900|          3900|            3900|           3900|              3900|          3900|                  3900|\n",
      "|   mean|            1950.5|44.06846153846154|  NULL|          NULL|       NULL|    59.76435897435898|    NULL|NULL|  NULL|  NULL| 3.749948717948712|               NULL|          NULL|            NULL|           NULL| 25.35153846153846|          NULL|                  NULL|\n",
      "| stddev|1125.9773532358456|15.20758912716237|  NULL|          NULL|       NULL|   23.685392250875328|    NULL|NULL|  NULL|  NULL|0.7162228139312412|               NULL|          NULL|            NULL|           NULL|14.447125170462305|          NULL|                  NULL|\n",
      "|    min|                 1|               18|Female|      Backpack|Accessories|                   20| Alabama|   L| Beige|  Fall|               2.5|                 No|2-Day Shipping|              No|             No|                 1| Bank Transfer|              Annually|\n",
      "|    max|              3900|               70|  Male|       T-shirt|  Outerwear|                  100| Wyoming|  XL|Yellow|Winter|               5.0|                Yes|  Store Pickup|             Yes|            Yes|                50|         Venmo|                Weekly|\n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data summary: \")\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97c6b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values: \n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "|Customer ID_unique|Age_unique|Gender_unique|Item Purchased_unique|Category_unique|Purchase Amount (USD)_unique|Location_unique|Size_unique|Color_unique|Season_unique|Review Rating_unique|Subscription Status_unique|Shipping Type_unique|Discount Applied_unique|Promo Code Used_unique|Previous Purchases_unique|Payment Method_unique|Frequency of Purchases_unique|\n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "|              3900|        53|            2|                   25|              4|                          81|             50|          4|          25|            4|                  26|                         2|                   6|                      2|                     2|                       50|                    6|                            7|\n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Number of unique values: \")\n",
    "data.select([countDistinct(col).alias(f\"{col}_unique\") for col in data.columns]).show()\n",
    "\n",
    "# for column in data.columns:\n",
    "#     print(f\"Column: {column}\")\n",
    "#     data.select(column).distinct().show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "196cb79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for nulls: \n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|Customer ID|Age|Gender|Item Purchased|Category|Purchase Amount (USD)|Location|Size|Color|Season|Review Rating|Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|          0|  0|     0|             0|       0|                    0|       0|   0|    0|     0|            0|                  0|            0|               0|              0|                 0|             0|                     0|\n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Check for nulls: \")\n",
    "null_nan_counts = data.select([\n",
    "    sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)).alias(c)\n",
    "    for c in data.columns\n",
    "])\n",
    "null_nan_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedc018",
   "metadata": {},
   "source": [
    "As we inspecting the data, we found that it's mainly about apparel products. It has good analytical features as,\n",
    "- No nulls\n",
    "- No outliers as seen in **Min** and **Max** of each column in the summary\n",
    "- Standardized format of values (e.g. Gender has 2 values (Male, Female) Which always appear in consistent formats, no M or F for example)\n",
    "\n",
    "However some issues were detected\n",
    "- Checking the Customer ID column, it doesn't have any useful indication, so it can be dropped. \n",
    "- Categorical data should be encoded for better machine learning model training (e.g., Yes/No to 1/0).\n",
    "- Numerical data should be standardized to avoid bias to certain features.\n",
    "- Binning columns as age into categories (e.g., '18-25', '26-35', etc.) can capture non-linear relationships and make the model more interpretable. The same applies for Purchase amount which can be Purchase Tier: (e.g., 'Low', 'Medium', 'High') since. We might then drop the original columns.\n",
    "- Finally, splitting the data for model train and test datasets should be done (Data is split first then standardized based on training set statistics to avoid data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d339e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns of irrelevant data\n",
    "data = data.drop(\"Customer ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning columns\n",
    "import preprocessing_utils as pf\n",
    "\n",
    "def bin_all_columns(row):\n",
    "    try:\n",
    "        new_age = pf.bin_age(row)\n",
    "        new_purchase_amount = pf.bin_purchase_amount(row)\n",
    "        new_previous_purchase = pf.bin_previous_purchases(row)\n",
    "\n",
    "        new_row = row.asDict()\n",
    "        new_row[\"Age\"] = new_age\n",
    "        new_row[\"Purchase Amount (USD)\"] = new_purchase_amount\n",
    "        new_row[\"Previous Purchases\"] = new_previous_purchase\n",
    "\n",
    "        return Row(**new_row)\n",
    "    except Exception as e:\n",
    "        print(\"Error processing row:\", row)\n",
    "        print(\"Error message:\", e)\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "rdd = data.rdd\n",
    "rdd = rdd.map(bin_all_columns)\n",
    "data = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39ff9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|  Age|Gender|Item Purchased|Category|Purchase Amount (USD)|     Location|Size|    Color|Season|Review Rating|Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-----+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|51-70|  Male|        Blouse|Clothing|               Medium|     Kentucky|   L|     Gray|Winter|          3.1|                Yes|      Express|             Yes|            Yes|             11-15|         Venmo|           Fortnightly|\n",
      "|18-25|  Male|       Sweater|Clothing|               Medium|        Maine|   L|   Maroon|Winter|          3.1|                Yes|      Express|             Yes|            Yes|               0-5|          Cash|           Fortnightly|\n",
      "|36-50|  Male|         Jeans|Clothing|                 High|Massachusetts|   S|   Maroon|Spring|          3.1|                Yes|Free Shipping|             Yes|            Yes|             21-25|   Credit Card|                Weekly|\n",
      "|18-25|  Male|       Sandals|Footwear|                 High| Rhode Island|   M|   Maroon|Spring|          3.5|                Yes| Next Day Air|             Yes|            Yes|             46-50|        PayPal|                Weekly|\n",
      "|36-50|  Male|        Blouse|Clothing|               Medium|       Oregon|   M|Turquoise|Spring|          2.7|                Yes|Free Shipping|             Yes|            Yes|             31-35|        PayPal|              Annually|\n",
      "+-----+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1821fc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+\n",
      "|Review Rating|Age         |Gender|Item Purchased                                                             |Category    |Purchase Amount (USD)|Location                                                                                                                                              |Size        |Color                                                                      |Season      |Subscription Status|Shipping Type     |Discount Applied|Promo Code Used|Previous Purchases            |Payment Method    |Frequency of Purchases|\n",
      "+-------------+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+\n",
      "|3.1          |[1, 0, 0, 0]|[1, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0]             |[1, 0, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0, 0] |\n",
      "|3.1          |[0, 1, 0, 0]|[1, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0]             |[1, 0, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0, 0] |\n",
      "|3.1          |[0, 0, 1, 0]|[1, 0]|[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[1, 0]             |[0, 1, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0, 0, 0]|[0, 1, 0, 0, 0, 0, 0] |\n",
      "|3.5          |[0, 1, 0, 0]|[1, 0]|[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[0, 1, 0]            |[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[1, 0]             |[0, 0, 1, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 1, 0, 0, 0, 0, 0] |\n",
      "|2.7          |[0, 0, 1, 0]|[1, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[1, 0]             |[0, 1, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 0, 1, 0, 0, 0, 0] |\n",
      "+-------------+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Categorical columns (all but not 'Review Rating')\n",
    "categorical_cols = data.columns\n",
    "categorical_cols.remove(\"Review Rating\")\n",
    "\n",
    "distinct_values = {}\n",
    "for col in categorical_cols:\n",
    "    distinct_values[col] = rdd.map(lambda row: row[col]).distinct().collect()\n",
    "\n",
    "# Index mappings for each categorical column\n",
    "category_to_index = {}\n",
    "for col in categorical_cols:\n",
    "    category_to_index[col] = {value: idx for idx, value in enumerate(distinct_values[col])}\n",
    "\n",
    "# Applying one-hot encoding and string indexing\n",
    "def encode_row(row):\n",
    "    # Convert Row to dictionary so we can modify it\n",
    "    row_dict = row.asDict()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        index = category_to_index[col].get(row[col], -1) \n",
    "        row_dict[col + \"_index\"] = index\n",
    "        \n",
    "        one_hot = [0] * len(distinct_values[col])\n",
    "        if index >= 0:\n",
    "            one_hot[index] = 1  # Set the corresponding index to 1\n",
    "        row_dict[col + \"_onehot\"] = one_hot\n",
    "    \n",
    "    return Row(**row_dict)\n",
    "\n",
    "# Apply the encoding logic with Map\n",
    "encoded_rdd = rdd.map(encode_row)\n",
    "encoded_data = spark.createDataFrame(encoded_rdd)\n",
    "\n",
    "encoded_data = encoded_data.drop(*([col + \"_index\" for col in categorical_cols] + [col for col in categorical_cols]))\n",
    "\n",
    "for col in encoded_data.columns:\n",
    "    if \"_onehot\" in col:\n",
    "        new_col = col.replace(\"_onehot\", \"\")\n",
    "        encoded_data = encoded_data.withColumnRenamed(col, new_col)\n",
    "\n",
    "# Show the result\n",
    "encoded_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e4378bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count:  3177\n",
      "Test data count:  723\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "|Age         |Gender|Item Purchased                                                             |Category    |Purchase Amount (USD)|Location                                                                                                                                              |Size        |Color                                                                      |Season      |Subscription Status|Shipping Type     |Discount Applied|Promo Code Used|Previous Purchases            |Payment Method    |Frequency of Purchases|Review Rating (scaled)|\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[0, 0, 0, 1]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1]             |[1, 0, 0, 0, 0, 0]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 0, 0, 0, 1, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]|[0, 0, 0, 1]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[0, 1, 0, 0]|[0, 1]             |[0, 0, 0, 0, 1, 0]|[0, 1]          |[0, 1]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 0, 1, 0]|[0, 0, 0, 0, 1, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[1, 0, 0, 0]|[0, 1]             |[0, 0, 0, 0, 0, 1]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]|[0, 0, 1, 0, 0, 0]|[0, 0, 0, 0, 0, 0, 1] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 1]             |[0, 0, 0, 1, 0, 0]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[0, 0, 0, 0, 0, 1]|[0, 0, 0, 0, 1, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]|[0, 1, 0, 0]|[0, 0, 1]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]|[0, 0, 1, 0]|[1, 0]             |[0, 0, 1, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 0, 0, 1]|[0, 1, 0, 0, 0, 0, 0] |-1.72554459347266     |\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "|Age         |Gender|Item Purchased                                                             |Category    |Purchase Amount (USD)|Location                                                                                                                                              |Size        |Color                                                                      |Season      |Subscription Status|Shipping Type     |Discount Applied|Promo Code Used|Previous Purchases            |Payment Method    |Frequency of Purchases|Review Rating (scaled)|\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1]|[1, 0, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]|[0, 0, 1, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1]|[0, 1]             |[0, 0, 1, 0, 0, 0]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0]|[0, 0, 0, 0, 0, 1, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]|[0, 0, 0, 1]|[1, 0, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[0, 1]             |[0, 0, 1, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]|[0, 1, 0, 0, 0, 0]|[0, 0, 0, 0, 0, 1, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 1]             |[0, 1, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0, 0, 0]|[0, 1, 0, 0, 0, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 1, 0]|[0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1]             |[0, 1, 0, 0, 0, 0]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 0, 0, 0, 0, 1, 0] |-1.72554459347266     |\n",
      "|[0, 0, 1, 0]|[0, 1]|[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 1]             |[0, 1, 0, 0, 0, 0]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]|[0, 0, 0, 0, 0, 1]|[0, 0, 0, 1, 0, 0, 0] |-1.72554459347266     |\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardizing numerical columns\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Assemble the numerical column into a vector\n",
    "assembler = VectorAssembler(inputCols=[\"Review Rating\"], outputCol=\"review_vector\")\n",
    "assembled_data = assembler.transform(encoded_data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = assembled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Training data count: \", train_data.count())\n",
    "print(\"Test data count: \", test_data.count())\n",
    "\n",
    "# Apply standardization\n",
    "scaler = StandardScaler(inputCol=\"review_vector\", outputCol=\"review_scaled\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(train_data)\n",
    "train_data = scaler_model.transform(train_data)\n",
    "test_data = scaler_model.transform(test_data)\n",
    "\n",
    "# Drop original scalar column and rename scaled vector\n",
    "train_data = train_data.drop(\"Review Rating\", \"review_vector\").withColumnRenamed(\"review_scaled\", \"Review Rating (scaled)\")\n",
    "\n",
    "train_data = train_data.withColumn(\"Review Rating (scaled)\", vector_to_array(\"Review Rating (scaled)\")[0])\n",
    "train_data.show(5, truncate=False)\n",
    "\n",
    "test_data = test_data.drop(\"Review Rating\", \"review_vector\").withColumnRenamed(\"review_scaled\", \"Review Rating (scaled)\")\n",
    "\n",
    "test_data = test_data.withColumn(\"Review Rating (scaled)\", vector_to_array(\"Review Rating (scaled)\")[0])\n",
    "test_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf19d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for target: Age\n",
      "f1 for Age: 0.4434\n",
      "weightedPrecision for Age: 0.7589\n",
      "weightedRecall for Age: 0.5934\n",
      "accuracy for Age: 0.5934\n",
      "\n",
      "Training model for target: Gender\n",
      "f1 for Gender: 0.7261\n",
      "weightedPrecision for Gender: 0.8472\n",
      "weightedRecall for Gender: 0.7206\n",
      "accuracy for Gender: 0.7206\n",
      "\n",
      "Training model for target: Item Purchased\n",
      "f1 for Item Purchased: 0.9484\n",
      "weightedPrecision for Item Purchased: 0.9320\n",
      "weightedRecall for Item Purchased: 0.9654\n",
      "accuracy for Item Purchased: 0.9654\n",
      "\n",
      "Training model for target: Category\n",
      "f1 for Category: 0.4877\n",
      "weightedPrecision for Category: 0.7242\n",
      "weightedRecall for Category: 0.6003\n",
      "accuracy for Category: 0.6003\n",
      "\n",
      "Training model for target: Purchase Amount (USD)\n",
      "f1 for Purchase Amount (USD): 0.5017\n",
      "weightedPrecision for Purchase Amount (USD): 0.4119\n",
      "weightedRecall for Purchase Amount (USD): 0.6418\n",
      "accuracy for Purchase Amount (USD): 0.6418\n",
      "\n",
      "Training model for target: Location\n",
      "f1 for Location: 0.9772\n",
      "weightedPrecision for Location: 0.9698\n",
      "weightedRecall for Location: 0.9848\n",
      "accuracy for Location: 0.9848\n",
      "\n",
      "Training model for target: Size\n",
      "f1 for Size: 0.6480\n",
      "weightedPrecision for Size: 0.5682\n",
      "weightedRecall for Size: 0.7538\n",
      "accuracy for Size: 0.7538\n",
      "\n",
      "Training model for target: Color\n",
      "f1 for Color: 0.9280\n",
      "weightedPrecision for Color: 0.9055\n",
      "weightedRecall for Color: 0.9516\n",
      "accuracy for Color: 0.9516\n",
      "\n",
      "Training model for target: Season\n",
      "f1 for Season: 0.6238\n",
      "weightedPrecision for Season: 0.5414\n",
      "weightedRecall for Season: 0.7358\n",
      "accuracy for Season: 0.7358\n",
      "\n",
      "Training model for target: Subscription Status\n",
      "f1 for Subscription Status: 0.8552\n",
      "weightedPrecision for Subscription Status: 0.9044\n",
      "weightedRecall for Subscription Status: 0.8451\n",
      "accuracy for Subscription Status: 0.8451\n",
      "\n",
      "Training model for target: Shipping Type\n",
      "f1 for Shipping Type: 0.7605\n",
      "weightedPrecision for Shipping Type: 0.6979\n",
      "weightedRecall for Shipping Type: 0.8354\n",
      "accuracy for Shipping Type: 0.8354\n",
      "\n",
      "Training model for target: Discount Applied\n",
      "f1 for Discount Applied: 1.0000\n",
      "weightedPrecision for Discount Applied: 1.0000\n",
      "weightedRecall for Discount Applied: 1.0000\n",
      "accuracy for Discount Applied: 1.0000\n",
      "\n",
      "Training model for target: Promo Code Used\n",
      "f1 for Promo Code Used: 1.0000\n",
      "weightedPrecision for Promo Code Used: 1.0000\n",
      "weightedRecall for Promo Code Used: 1.0000\n",
      "accuracy for Promo Code Used: 1.0000\n",
      "\n",
      "Training model for target: Previous Purchases\n",
      "f1 for Previous Purchases: 0.8532\n",
      "weightedPrecision for Previous Purchases: 0.8107\n",
      "weightedRecall for Previous Purchases: 0.9004\n",
      "accuracy for Previous Purchases: 0.9004\n",
      "\n",
      "Training model for target: Payment Method\n",
      "f1 for Payment Method: 0.7878\n",
      "weightedPrecision for Payment Method: 0.7306\n",
      "weightedRecall for Payment Method: 0.8548\n",
      "accuracy for Payment Method: 0.8548\n",
      "\n",
      "Training model for target: Frequency of Purchases\n",
      "f1 for Frequency of Purchases: 0.8075\n",
      "weightedPrecision for Frequency of Purchases: 0.7545\n",
      "weightedRecall for Frequency of Purchases: 0.8686\n",
      "accuracy for Frequency of Purchases: 0.8686\n",
      "\n",
      "Training model for target: Review Rating (scaled)\n",
      "f1 for Review Rating (scaled): 1.0000\n",
      "weightedPrecision for Review Rating (scaled): 1.0000\n",
      "weightedRecall for Review Rating (scaled): 1.0000\n",
      "accuracy for Review Rating (scaled): 1.0000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# UDF to convert array to a single double value\n",
    "@udf(returnType=DoubleType())\n",
    "def array_to_double(arr):\n",
    "    if arr is None:\n",
    "        return None\n",
    "    try:\n",
    "        return float(arr[0]) if len(arr) > 0 else 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Identify all columns to flatten (exclude 'review_vector')\n",
    "array_cols = [col for col in train_data.columns if col != \"review_vector\"]\n",
    "\n",
    "# Flatten arrays\n",
    "flat_train_data = train_data.select(\n",
    "    *[array_to_double(col).alias(f\"{col}_flat\") if col != \"review_vector\" else col for col in array_cols]\n",
    ")\n",
    "flat_test_data = test_data.select(\n",
    "    *[array_to_double(col).alias(f\"{col}_flat\") if col != \"review_vector\" else col for col in array_cols]\n",
    ")\n",
    "\n",
    "# List of all target columns\n",
    "target_cols = [col for col in array_cols if col != \"review_vector\"]  # or provide explicitly\n",
    "\n",
    "for target_col in target_cols:\n",
    "    print(f\"\\nTraining model for target: {target_col}\")\n",
    "\n",
    "    # Feature columns exclude current target and review_vector\n",
    "    feature_cols = [col for col in flat_train_data.columns if col not in [f\"{target_col}_flat\", \"review_vector\"] and col.endswith(\"_flat\")]\n",
    "\n",
    "    # Assemble features\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    train_assembled = assembler.transform(flat_train_data.select(*feature_cols, f\"{target_col}_flat\"))\n",
    "    test_assembled = assembler.transform(flat_test_data.select(*feature_cols, f\"{target_col}_flat\"))\n",
    "\n",
    "    # Determine task type\n",
    "    if target_col == \"Review Rating\":\n",
    "        model = RandomForestRegressor(labelCol=f\"{target_col}_flat\", featuresCol=\"features\")\n",
    "        evaluator = RegressionEvaluator(labelCol=f\"{target_col}_flat\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    else:\n",
    "        model = RandomForestClassifier(labelCol=f\"{target_col}_flat\", featuresCol=\"features\")\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=f\"{target_col}_flat\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    # Fit and predict\n",
    "    fitted_model = model.fit(train_assembled)\n",
    "    predictions = fitted_model.transform(test_assembled)\n",
    "\n",
    "    # Evaluate\n",
    "    if target_col != \"Review Rating\":\n",
    "        # Classification metrics\n",
    "        for metric in [\"f1\", \"weightedPrecision\", \"weightedRecall\", \"accuracy\"]:\n",
    "            evaluator.setMetricName(metric)\n",
    "            score = evaluator.evaluate(predictions)\n",
    "            print(f\"{metric} for {target_col}: {score:.4f}\")\n",
    "    else:\n",
    "        # Regression metric\n",
    "        score = evaluator.evaluate(predictions)\n",
    "        print(f\"RMSE for {target_col}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
