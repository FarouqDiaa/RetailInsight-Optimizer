{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6422b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import col, countDistinct, isnan, when, sum\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark_home = \"C:\\\\Program Files\\\\ApacheSpark\"\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "\n",
    "# Add Spark bin and executors to PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"bin\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"sbin\")\n",
    "\n",
    "# Add Spark Python libraries to PYTHONPATH\n",
    "os.environ[\"PYTHONPATH\"] = os.path.join(spark_home, \"python\") + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PYTHONPATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\")\n",
    "\n",
    "# Add PySpark to the system path\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"pyspark.zip\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9-src.zip\")\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b15f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailInsight-Optimizer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6ce84",
   "metadata": {},
   "source": [
    "# Load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a40b2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../data/data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34197f8",
   "metadata": {},
   "source": [
    "# Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c380339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  3900\n",
      "Sample data: \n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|Customer ID|Age|Gender|Item Purchased|Category|Purchase Amount (USD)|     Location|Size|    Color|Season|Review Rating|Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|          1| 55|  Male|        Blouse|Clothing|                   53|     Kentucky|   L|     Gray|Winter|          3.1|                Yes|      Express|             Yes|            Yes|                14|         Venmo|           Fortnightly|\n",
      "|          2| 19|  Male|       Sweater|Clothing|                   64|        Maine|   L|   Maroon|Winter|          3.1|                Yes|      Express|             Yes|            Yes|                 2|          Cash|           Fortnightly|\n",
      "|          3| 50|  Male|         Jeans|Clothing|                   73|Massachusetts|   S|   Maroon|Spring|          3.1|                Yes|Free Shipping|             Yes|            Yes|                23|   Credit Card|                Weekly|\n",
      "|          4| 21|  Male|       Sandals|Footwear|                   90| Rhode Island|   M|   Maroon|Spring|          3.5|                Yes| Next Day Air|             Yes|            Yes|                49|        PayPal|                Weekly|\n",
      "|          5| 45|  Male|        Blouse|Clothing|                   49|       Oregon|   M|Turquoise|Spring|          2.7|                Yes|Free Shipping|             Yes|            Yes|                31|        PayPal|              Annually|\n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records: \", data.rdd.count())\n",
    "\n",
    "print(\"Sample data: \")\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e62372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data schema: \n",
      "root\n",
      " |-- Customer ID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Item Purchased: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Purchase Amount (USD): integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Review Rating: double (nullable = true)\n",
      " |-- Subscription Status: string (nullable = true)\n",
      " |-- Shipping Type: string (nullable = true)\n",
      " |-- Discount Applied: string (nullable = true)\n",
      " |-- Promo Code Used: string (nullable = true)\n",
      " |-- Previous Purchases: integer (nullable = true)\n",
      " |-- Payment Method: string (nullable = true)\n",
      " |-- Frequency of Purchases: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Data schema: \")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01317b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary: \n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|summary|       Customer ID|              Age|Gender|Item Purchased|   Category|Purchase Amount (USD)|Location|Size| Color|Season|     Review Rating|Subscription Status| Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|  count|              3900|             3900|  3900|          3900|       3900|                 3900|    3900|3900|  3900|  3900|              3900|               3900|          3900|            3900|           3900|              3900|          3900|                  3900|\n",
      "|   mean|            1950.5|44.06846153846154|  NULL|          NULL|       NULL|    59.76435897435898|    NULL|NULL|  NULL|  NULL| 3.749948717948712|               NULL|          NULL|            NULL|           NULL| 25.35153846153846|          NULL|                  NULL|\n",
      "| stddev|1125.9773532358456|15.20758912716237|  NULL|          NULL|       NULL|   23.685392250875328|    NULL|NULL|  NULL|  NULL|0.7162228139312412|               NULL|          NULL|            NULL|           NULL|14.447125170462305|          NULL|                  NULL|\n",
      "|    min|                 1|               18|Female|      Backpack|Accessories|                   20| Alabama|   L| Beige|  Fall|               2.5|                 No|2-Day Shipping|              No|             No|                 1| Bank Transfer|              Annually|\n",
      "|    max|              3900|               70|  Male|       T-shirt|  Outerwear|                  100| Wyoming|  XL|Yellow|Winter|               5.0|                Yes|  Store Pickup|             Yes|            Yes|                50|         Venmo|                Weekly|\n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data summary: \")\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c6b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values: \n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "|Customer ID_unique|Age_unique|Gender_unique|Item Purchased_unique|Category_unique|Purchase Amount (USD)_unique|Location_unique|Size_unique|Color_unique|Season_unique|Review Rating_unique|Subscription Status_unique|Shipping Type_unique|Discount Applied_unique|Promo Code Used_unique|Previous Purchases_unique|Payment Method_unique|Frequency of Purchases_unique|\n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "|              3900|        53|            2|                   25|              4|                          81|             50|          4|          25|            4|                  26|                         2|                   6|                      2|                     2|                       50|                    6|                            7|\n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Number of unique values: \")\n",
    "data.select([countDistinct(col).alias(f\"{col}_unique\") for col in data.columns]).show()\n",
    "\n",
    "# for column in data.columns:\n",
    "#     print(f\"Column: {column}\")\n",
    "#     data.select(column).distinct().show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196cb79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for nulls: \n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|Customer ID|Age|Gender|Item Purchased|Category|Purchase Amount (USD)|Location|Size|Color|Season|Review Rating|Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|          0|  0|     0|             0|       0|                    0|       0|   0|    0|     0|            0|                  0|            0|               0|              0|                 0|             0|                     0|\n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Check for nulls: \")\n",
    "null_nan_counts = data.select([\n",
    "    sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)).alias(c)\n",
    "    for c in data.columns\n",
    "])\n",
    "null_nan_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedc018",
   "metadata": {},
   "source": [
    "As we inspecting the data, we found that it's mainly about apparel products. It has good analytical features as,\n",
    "- No nulls\n",
    "- No outliers as seen in **Min** and **Max** of each column in the summary\n",
    "- Standardized format of values (e.g. Gender has 2 values (Male, Female) Which always appear in consistent formats, no M or F for example)\n",
    "\n",
    "However some issues were detected\n",
    "- Checking the Customer ID column, it doesn't have any useful indication, so it can be dropped. \n",
    "- Categorical data should be encoded for better machine learning model training (e.g., Yes/No to 1/0).\n",
    "- Numerical data should be standardized to avoid bias to certain features.\n",
    "- Binning columns as age into categories (e.g., '18-25', '26-35', etc.) can capture non-linear relationships and make the model more interpretable. The same applies for Purchase amount which can be Purchase Tier: (e.g., 'Low', 'Medium', 'High') since. We might then drop the original columns.\n",
    "- Finally, splitting the data for model train and test datasets should be done (Data is split first then standardized based on training set statistics to avoid data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d339e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns of irrelevant data\n",
    "data = data.drop(\"Customer ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning columns\n",
    "import preprocessing_utils as pf\n",
    "\n",
    "def bin_all_columns(row):\n",
    "    try:\n",
    "        new_age = pf.bin_age(row)\n",
    "        new_purchase_amount = pf.bin_purchase_amount(row)\n",
    "        new_previous_purchase = pf.bin_previous_purchases(row)\n",
    "\n",
    "        new_row = row.asDict()\n",
    "        new_row[\"Age\"] = new_age\n",
    "        new_row[\"Purchase Amount (USD)\"] = new_purchase_amount\n",
    "        new_row[\"Previous Purchases\"] = new_previous_purchase\n",
    "\n",
    "        return Row(**new_row)\n",
    "    except Exception as e:\n",
    "        print(\"Error processing row:\", row)\n",
    "        print(\"Error message:\", e)\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "rdd = data.rdd\n",
    "rdd = rdd.map(bin_all_columns)\n",
    "data = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1821fc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+\n",
      "|Review Rating|Age         |Gender|Item Purchased                                                             |Category    |Purchase Amount (USD)|Location                                                                                                                                              |Size        |Color                                                                      |Season      |Subscription Status|Shipping Type     |Discount Applied|Promo Code Used|Previous Purchases            |Payment Method    |Frequency of Purchases|\n",
      "+-------------+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+\n",
      "|3.1          |[1, 0, 0, 0]|[1, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0]             |[1, 0, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0, 0] |\n",
      "|3.1          |[0, 1, 0, 0]|[1, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0]             |[1, 0, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0, 0, 0]|[1, 0, 0, 0, 0, 0, 0] |\n",
      "|3.1          |[0, 0, 1, 0]|[1, 0]|[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[1, 0]             |[0, 1, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0, 0, 0]|[0, 1, 0, 0, 0, 0, 0] |\n",
      "|3.5          |[0, 1, 0, 0]|[1, 0]|[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[0, 1, 0]            |[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[1, 0]             |[0, 0, 1, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 1, 0, 0, 0, 0, 0] |\n",
      "|2.7          |[0, 0, 1, 0]|[1, 0]|[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[1, 0, 0]            |[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 1, 0, 0]|[1, 0]             |[0, 1, 0, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 0, 1, 0, 0, 0, 0] |\n",
      "+-------------+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Categorical columns (all but not 'Review Rating')\n",
    "categorical_cols = data.columns\n",
    "categorical_cols.remove(\"Review Rating\")\n",
    "\n",
    "distinct_values = {}\n",
    "for col in categorical_cols:\n",
    "    distinct_values[col] = rdd.map(lambda row: row[col]).distinct().collect()\n",
    "\n",
    "# Index mappings for each categorical column\n",
    "category_to_index = {}\n",
    "for col in categorical_cols:\n",
    "    category_to_index[col] = {value: idx for idx, value in enumerate(distinct_values[col])}\n",
    "\n",
    "# Applying one-hot encoding and string indexing\n",
    "def encode_row(row):    \n",
    "    for col in categorical_cols:\n",
    "        index = category_to_index[col].get(row[col], -1) \n",
    "        row[col + \"_index\"] = index\n",
    "        \n",
    "        one_hot = [0] * len(distinct_values[col])\n",
    "        if index >= 0:\n",
    "            one_hot[index] = 1  # Set the corresponding index to 1\n",
    "        row[col + \"_onehot\"] = one_hot\n",
    "    \n",
    "    return Row(**row)\n",
    "\n",
    "# Apply the encoding logic with Map\n",
    "encoded_rdd = rdd.map(encode_row)\n",
    "encoded_data = spark.createDataFrame(encoded_rdd)\n",
    "\n",
    "encoded_data = encoded_data.drop(*([col + \"_index\" for col in categorical_cols] + [col for col in categorical_cols]))\n",
    "\n",
    "for col in encoded_data.columns:\n",
    "    if \"_onehot\" in col:\n",
    "        new_col = col.replace(\"_onehot\", \"\")\n",
    "        encoded_data = encoded_data.withColumnRenamed(col, new_col)\n",
    "\n",
    "# Show the result\n",
    "encoded_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e4378bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count:  3177\n",
      "Test data count:  723\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "|Age         |Gender|Item Purchased                                                             |Category    |Purchase Amount (USD)|Location                                                                                                                                              |Size        |Color                                                                      |Season      |Subscription Status|Shipping Type     |Discount Applied|Promo Code Used|Previous Purchases            |Payment Method    |Frequency of Purchases|Review Rating (scaled)|\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[0, 0, 0, 1]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1]             |[1, 0, 0, 0, 0, 0]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]|[0, 0, 0, 1, 0, 0]|[0, 0, 0, 0, 1, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]|[0, 0, 0, 1]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[0, 1, 0, 0]|[0, 1]             |[0, 0, 0, 0, 1, 0]|[0, 1]          |[0, 1]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 0, 1, 0]|[0, 0, 0, 0, 1, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 1]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[1, 0, 0, 0]|[0, 1]             |[0, 0, 0, 0, 0, 1]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]|[0, 0, 1, 0, 0, 0]|[0, 0, 0, 0, 0, 0, 1] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[0, 1]|[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 1, 0]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]|[0, 0, 1, 0]|[0, 1]             |[0, 0, 0, 1, 0, 0]|[0, 1]          |[0, 1]         |[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]|[0, 0, 0, 0, 0, 1]|[0, 0, 0, 0, 1, 0, 0] |-1.72554459347266     |\n",
      "|[0, 0, 0, 1]|[1, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]|[0, 1, 0, 0]|[0, 0, 1]            |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|[1, 0, 0, 0]|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]|[0, 0, 1, 0]|[1, 0]             |[0, 0, 1, 0, 0, 0]|[1, 0]          |[1, 0]         |[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]|[0, 0, 0, 0, 0, 1]|[0, 1, 0, 0, 0, 0, 0] |-1.72554459347266     |\n",
      "+------------+------+---------------------------------------------------------------------------+------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------------------------------------------------------------------------+------------+-------------------+------------------+----------------+---------------+------------------------------+------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardizing numerical columns\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Assemble the numerical column into a vector\n",
    "assembler = VectorAssembler(inputCols=[\"Review Rating\"], outputCol=\"review_vector\")\n",
    "assembled_data = assembler.transform(encoded_data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = assembled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Training data count: \", train_data.count())\n",
    "print(\"Test data count: \", test_data.count())\n",
    "\n",
    "# Apply standardization\n",
    "scaler = StandardScaler(inputCol=\"review_vector\", outputCol=\"review_scaled\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(train_data)\n",
    "scaled_data = scaler_model.transform(train_data)\n",
    "scaled_test = scaler_model.transform(test_data)\n",
    "\n",
    "# Drop original scalar column and rename scaled vector\n",
    "scaled_data = scaled_data.drop(\"Review Rating\", \"review_vector\").withColumnRenamed(\"review_scaled\", \"Review Rating (scaled)\")\n",
    "\n",
    "scaled_data = scaled_data.withColumn(\"Review Rating (scaled)\", vector_to_array(\"Review Rating (scaled)\")[0])\n",
    "scaled_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final datasets\n",
    "# test_data, train_data\n",
    "# Final datasets\n",
    "# test_data, train_data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from scipy.stats import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "train_data = scaled_data.toPandas()\n",
    "test_data = scaled_test.toPandas()\n",
    "\n",
    "# Drop non-informative columns\n",
    "train_data = train_data.drop(columns=[ \"Discount Applied\"])\n",
    "test_data = test_data.drop(columns=[ \"Discount Applied\"])\n",
    "\n",
    "\n",
    "\n",
    "# Identify categorical vs numerical targets\n",
    "categorical_cols = train_data.columns\n",
    "numerical_cols = train_data.columns['Review Rating (scaled)']\n",
    "\n",
    "# Set up parameter space for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 300),\n",
    "    'max_depth': randint(5, 30),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Train a model for each target column\n",
    "results = {}\n",
    "\n",
    "for target_col in train_data.columns:\n",
    "    feature_cols = train_data.columns[train_data.columns != target_col]\n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data[target_col]\n",
    "    X_test = test_data[feature_cols]\n",
    "    y_test = test_data[target_col]\n",
    "\n",
    "    # Choose model type\n",
    "    if target_col in numerical_cols:\n",
    "        base_model = RandomForestRegressor(random_state=42)\n",
    "        metric_name = \"MSE\"\n",
    "    else:\n",
    "        base_model = RandomForestClassifier(random_state=42)\n",
    "        metric_name = \"Accuracy\"\n",
    "\n",
    "    # Randomized hyperparameter tuning\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        cv=3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if target_col in numerical_cols:\n",
    "        score = mean_squared_error(y_test, y_pred)\n",
    "    else:\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        print(f\"\\nClassification Report for {target_col}:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    results[target_col] = f\"{metric_name}: {score:.4f}\"\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n=== Summary Results ===\")\n",
    "for col, score in results.items():\n",
    "    print(f\"{col}: {score}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
