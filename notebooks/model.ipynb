{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6422b51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfindspark\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mfindspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Vectors\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\findspark.py:143\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(spark_home, python_path, edit_rc, edit_profile)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make pyspark importable.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m \u001b[33;03mSets environment variables and adds dependencies to sys.path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m \u001b[33;03m    configure and import pyspark.\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spark_home:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     spark_home = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m python_path:\n\u001b[32m    146\u001b[39m     python_path = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYSPARK_PYTHON\u001b[39m\u001b[33m\"\u001b[39m, sys.executable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\findspark.py:46\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     43\u001b[39m         spark_home = os.path.dirname(pyspark.\u001b[34m__file__\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spark_home:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     47\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find Spark, make sure SPARK_HOME env is set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m or Spark is in an expected location (e.g. from homebrew installation).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m     )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m spark_home\n",
      "\u001b[31mValueError\u001b[39m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import col, countDistinct, isnan, when, sum\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark_home = \"C:\\\\Program Files\\\\ApacheSpark\"\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "\n",
    "# Add Spark bin and executors to PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"bin\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"sbin\")\n",
    "\n",
    "# Add Spark Python libraries to PYTHONPATH\n",
    "os.environ[\"PYTHONPATH\"] = os.path.join(spark_home, \"python\") + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PYTHONPATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\")\n",
    "\n",
    "# Add PySpark to the system path\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"pyspark.zip\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9-src.zip\")\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailInsight-Optimizer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6ce84",
   "metadata": {},
   "source": [
    "# Load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../data/data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34197f8",
   "metadata": {},
   "source": [
    "# Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c380339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  3900\n",
      "Sample data: \n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|Customer ID|Age|Gender|Item Purchased|Category|Purchase Amount (USD)|     Location|Size|    Color|Season|Review Rating|Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|          1| 55|  Male|        Blouse|Clothing|                   53|     Kentucky|   L|     Gray|Winter|          3.1|                Yes|      Express|             Yes|            Yes|                14|         Venmo|           Fortnightly|\n",
      "|          2| 19|  Male|       Sweater|Clothing|                   64|        Maine|   L|   Maroon|Winter|          3.1|                Yes|      Express|             Yes|            Yes|                 2|          Cash|           Fortnightly|\n",
      "|          3| 50|  Male|         Jeans|Clothing|                   73|Massachusetts|   S|   Maroon|Spring|          3.1|                Yes|Free Shipping|             Yes|            Yes|                23|   Credit Card|                Weekly|\n",
      "|          4| 21|  Male|       Sandals|Footwear|                   90| Rhode Island|   M|   Maroon|Spring|          3.5|                Yes| Next Day Air|             Yes|            Yes|                49|        PayPal|                Weekly|\n",
      "|          5| 45|  Male|        Blouse|Clothing|                   49|       Oregon|   M|Turquoise|Spring|          2.7|                Yes|Free Shipping|             Yes|            Yes|                31|        PayPal|              Annually|\n",
      "+-----------+---+------+--------------+--------+---------------------+-------------+----+---------+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records: \", data.rdd.count())\n",
    "\n",
    "print(\"Sample data: \")\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e62372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data schema: \n",
      "root\n",
      " |-- Customer ID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Item Purchased: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Purchase Amount (USD): integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Review Rating: double (nullable = true)\n",
      " |-- Subscription Status: string (nullable = true)\n",
      " |-- Shipping Type: string (nullable = true)\n",
      " |-- Discount Applied: string (nullable = true)\n",
      " |-- Promo Code Used: string (nullable = true)\n",
      " |-- Previous Purchases: integer (nullable = true)\n",
      " |-- Payment Method: string (nullable = true)\n",
      " |-- Frequency of Purchases: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Data schema: \")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01317b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary: \n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|summary|       Customer ID|              Age|Gender|Item Purchased|   Category|Purchase Amount (USD)|Location|Size| Color|Season|     Review Rating|Subscription Status| Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|  count|              3900|             3900|  3900|          3900|       3900|                 3900|    3900|3900|  3900|  3900|              3900|               3900|          3900|            3900|           3900|              3900|          3900|                  3900|\n",
      "|   mean|            1950.5|44.06846153846154|  NULL|          NULL|       NULL|    59.76435897435898|    NULL|NULL|  NULL|  NULL| 3.749948717948712|               NULL|          NULL|            NULL|           NULL| 25.35153846153846|          NULL|                  NULL|\n",
      "| stddev|1125.9773532358456|15.20758912716237|  NULL|          NULL|       NULL|   23.685392250875328|    NULL|NULL|  NULL|  NULL|0.7162228139312412|               NULL|          NULL|            NULL|           NULL|14.447125170462305|          NULL|                  NULL|\n",
      "|    min|                 1|               18|Female|      Backpack|Accessories|                   20| Alabama|   L| Beige|  Fall|               2.5|                 No|2-Day Shipping|              No|             No|                 1| Bank Transfer|              Annually|\n",
      "|    max|              3900|               70|  Male|       T-shirt|  Outerwear|                  100| Wyoming|  XL|Yellow|Winter|               5.0|                Yes|  Store Pickup|             Yes|            Yes|                50|         Venmo|                Weekly|\n",
      "+-------+------------------+-----------------+------+--------------+-----------+---------------------+--------+----+------+------+------------------+-------------------+--------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data summary: \")\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values: \n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "|Customer ID_unique|Age_unique|Gender_unique|Item Purchased_unique|Category_unique|Purchase Amount (USD)_unique|Location_unique|Size_unique|Color_unique|Season_unique|Review Rating_unique|Subscription Status_unique|Shipping Type_unique|Discount Applied_unique|Promo Code Used_unique|Previous Purchases_unique|Payment Method_unique|Frequency of Purchases_unique|\n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "|              3900|        53|            2|                   25|              4|                          81|             50|          4|          25|            4|                  26|                         2|                   6|                      2|                     2|                       50|                    6|                            7|\n",
      "+------------------+----------+-------------+---------------------+---------------+----------------------------+---------------+-----------+------------+-------------+--------------------+--------------------------+--------------------+-----------------------+----------------------+-------------------------+---------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Number of unique values: \")\n",
    "data.select([countDistinct(col).alias(f\"{col}_unique\") for col in data.columns]).show()\n",
    "\n",
    "# for column in data.columns:\n",
    "#     print(f\"Column: {column}\")\n",
    "#     data.select(column).distinct().show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cb79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for nulls: \n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|Customer ID|Age|Gender|Item Purchased|Category|Purchase Amount (USD)|Location|Size|Color|Season|Review Rating|Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|          0|  0|     0|             0|       0|                    0|       0|   0|    0|     0|            0|                  0|            0|               0|              0|                 0|             0|                     0|\n",
      "+-----------+---+------+--------------+--------+---------------------+--------+----+-----+------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Check for nulls: \")\n",
    "null_nan_counts = data.select([\n",
    "    sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)).alias(c)\n",
    "    for c in data.columns\n",
    "])\n",
    "null_nan_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedc018",
   "metadata": {},
   "source": [
    "As we inspecting the data, we found that it's mainly about apparel products. It has good analytical features as,\n",
    "- No nulls\n",
    "- No outliers as seen in **Min** and **Max** of each column in the summary\n",
    "- Standardized format of values (e.g. Gender has 2 values (Male, Female) Which always appear in consistent formats, no M or F for example)\n",
    "\n",
    "However some issues were detected\n",
    "- Checking the Customer ID column, it doesn't have any useful indication, so it can be dropped. \n",
    "- Categorical data should be encoded for better machine learning model training (e.g., Yes/No to 1/0).\n",
    "- Numerical data should be standardized to avoid bias to certain features.\n",
    "- Binning columns as age into categories (e.g., '18-25', '26-35', etc.) can capture non-linear relationships and make the model more interpretable. The same applies for Purchase amount which can be Purchase Tier: (e.g., 'Low', 'Medium', 'High') since. We might then drop the original columns.\n",
    "- Finally, splitting the data for model train and test datasets should be done (Data is split first then standardized based on training set statistics to avoid data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d339e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns of irrelevant data\n",
    "data = data.drop(\"Customer ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning columns\n",
    "import preprocessing_utils as pf\n",
    "\n",
    "def bin_all_columns(row):\n",
    "    try:\n",
    "        new_age = pf.bin_age(row)\n",
    "        new_purchase_amount = pf.bin_purchase_amount(row)\n",
    "        new_previous_purchase = pf.bin_previous_purchases(row)\n",
    "\n",
    "        new_row = row.asDict()\n",
    "        new_row[\"Age\"] = new_age\n",
    "        new_row[\"Purchase Amount (USD)\"] = new_purchase_amount\n",
    "        new_row[\"Previous Purchases\"] = new_previous_purchase\n",
    "\n",
    "        return Row(**new_row)\n",
    "    except Exception as e:\n",
    "        print(\"Error processing row:\", row)\n",
    "        print(\"Error message:\", e)\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "rdd = data.rdd\n",
    "rdd = rdd.map(bin_all_columns)\n",
    "data = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821fc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+---------------+-------------+---------------------+---------------+-------------+---------------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|Review Rating|Age          |Gender       |Item Purchased |Category     |Purchase Amount (USD)|Location       |Size         |Color          |Season       |Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|\n",
      "+-------------+-------------+-------------+---------------+-------------+---------------------+---------------+-------------+---------------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "|3.1          |(3,[0],[1.0])|(1,[0],[1.0])|(24,[0],[1.0]) |(3,[0],[1.0])|(2,[1],[1.0])        |(49,[21],[1.0])|(3,[1],[1.0])|(24,[8],[1.0]) |(3,[2],[1.0])|(1,[],[])          |(5,[4],[1.0])|(1,[],[])       |(1,[],[])      |(9,[3],[1.0])     |(5,[4],[1.0]) |(6,[5],[1.0])         |\n",
      "|3.1          |(3,[],[])    |(1,[0],[1.0])|(24,[5],[1.0]) |(3,[0],[1.0])|(2,[1],[1.0])        |(49,[24],[1.0])|(3,[1],[1.0])|(24,[9],[1.0]) |(3,[2],[1.0])|(1,[],[])          |(5,[4],[1.0])|(1,[],[])       |(1,[],[])      |(9,[0],[1.0])     |(5,[2],[1.0]) |(6,[5],[1.0])         |\n",
      "|3.1          |(3,[1],[1.0])|(1,[0],[1.0])|(24,[],[])     |(3,[0],[1.0])|(2,[0],[1.0])        |(49,[38],[1.0])|(3,[2],[1.0])|(24,[9],[1.0]) |(3,[0],[1.0])|(1,[],[])          |(5,[0],[1.0])|(1,[],[])       |(1,[],[])      |(9,[1],[1.0])     |(5,[1],[1.0]) |(6,[],[])             |\n",
      "|3.5          |(3,[],[])    |(1,[0],[1.0])|(24,[10],[1.0])|(3,[2],[1.0])|(2,[0],[1.0])        |(49,[],[])     |(3,[0],[1.0])|(24,[9],[1.0]) |(3,[0],[1.0])|(1,[],[])          |(5,[3],[1.0])|(1,[],[])       |(1,[],[])      |(9,[4],[1.0])     |(5,[0],[1.0]) |(6,[],[])             |\n",
      "|2.7          |(3,[1],[1.0])|(1,[0],[1.0])|(24,[0],[1.0]) |(3,[0],[1.0])|(2,[1],[1.0])        |(49,[33],[1.0])|(3,[0],[1.0])|(24,[21],[1.0])|(3,[0],[1.0])|(1,[],[])          |(5,[0],[1.0])|(1,[],[])       |(1,[],[])      |(9,[2],[1.0])     |(5,[0],[1.0]) |(6,[1],[1.0])         |\n",
      "+-------------+-------------+-------------+---------------+-------------+---------------------+---------------+-------------+---------------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical variables\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    " # 1. Get distinct values for each categorical column\n",
    "categorical_cols = data.columns\n",
    "categorical_cols.remove(\"Review Rating\")\n",
    "\n",
    "# 1. StringIndexer stages\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\") for c in categorical_cols]\n",
    "\n",
    "# 2. OneHotEncoder stages\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_onehot\") for c in categorical_cols]\n",
    "\n",
    "# 3. Combine in Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "model = pipeline.fit(data)\n",
    "encoded_data = model.transform(data)\n",
    "\n",
    "encoded_data.select(*categorical_cols, *[c+\"_index\" for c in categorical_cols], *[c+\"_onehot\" for c in categorical_cols])\n",
    "# Drop original categorical columns\n",
    "encoded_data = encoded_data.drop(*categorical_cols, *[c+\"_index\" for c in categorical_cols])\n",
    "\n",
    "# Rename columns\n",
    "for col in encoded_data.columns:\n",
    "    if \"_onehot\" in col:\n",
    "        new_col = col.replace(\"_onehot\", \"\")\n",
    "        encoded_data = encoded_data.withColumnRenamed(col, new_col)\n",
    "\n",
    "encoded_data.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4378bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count:  3177\n",
      "Test data count:  723\n",
      "+---------+-------------+---------------+-------------+---------------------+---------------+-------------+---------------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+----------------------+\n",
      "|Age      |Gender       |Item Purchased |Category     |Purchase Amount (USD)|Location       |Size         |Color          |Season       |Subscription Status|Shipping Type|Discount Applied|Promo Code Used|Previous Purchases|Payment Method|Frequency of Purchases|Review Rating (scaled)|\n",
      "+---------+-------------+---------------+-------------+---------------------+---------------+-------------+---------------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+----------------------+\n",
      "|(3,[],[])|(1,[0],[1.0])|(24,[5],[1.0]) |(3,[0],[1.0])|(2,[],[])            |(49,[11],[1.0])|(3,[0],[1.0])|(24,[],[])     |(3,[2],[1.0])|(1,[0],[1.0])      |(5,[3],[1.0])|(1,[],[])       |(1,[],[])      |(9,[2],[1.0])     |(5,[1],[1.0]) |(6,[4],[1.0])         |-1.72554459347266     |\n",
      "|(3,[],[])|(1,[0],[1.0])|(24,[6],[1.0]) |(3,[],[])    |(2,[],[])            |(49,[23],[1.0])|(3,[0],[1.0])|(24,[18],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])      |(5,[1],[1.0])|(1,[],[])       |(1,[],[])      |(9,[1],[1.0])     |(5,[3],[1.0]) |(6,[3],[1.0])         |-1.72554459347266     |\n",
      "|(3,[],[])|(1,[0],[1.0])|(24,[7],[1.0]) |(3,[1],[1.0])|(2,[1],[1.0])        |(49,[38],[1.0])|(3,[2],[1.0])|(24,[0],[1.0]) |(3,[2],[1.0])|(1,[0],[1.0])      |(5,[],[])    |(1,[0],[1.0])   |(1,[0],[1.0])  |(9,[6],[1.0])     |(5,[3],[1.0]) |(6,[],[])             |-1.72554459347266     |\n",
      "|(3,[],[])|(1,[0],[1.0])|(24,[11],[1.0])|(3,[0],[1.0])|(2,[1],[1.0])        |(49,[8],[1.0]) |(3,[1],[1.0])|(24,[20],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])      |(5,[2],[1.0])|(1,[],[])       |(1,[],[])      |(9,[6],[1.0])     |(5,[2],[1.0]) |(6,[0],[1.0])         |-1.72554459347266     |\n",
      "|(3,[],[])|(1,[0],[1.0])|(24,[13],[1.0])|(3,[1],[1.0])|(2,[],[])            |(49,[6],[1.0]) |(3,[],[])    |(24,[12],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])      |(5,[2],[1.0])|(1,[],[])       |(1,[],[])      |(9,[1],[1.0])     |(5,[4],[1.0]) |(6,[0],[1.0])         |-1.72554459347266     |\n",
      "+---------+-------------+---------------+-------------+---------------------+---------------+-------------+---------------+-------------+-------------------+-------------+----------------+---------------+------------------+--------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardizing numerical columns\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Assemble the numerical column into a vector\n",
    "assembler = VectorAssembler(inputCols=[\"Review Rating\"], outputCol=\"review_vector\")\n",
    "assembled_data = assembler.transform(encoded_data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = assembled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Training data count: \", train_data.count())\n",
    "print(\"Test data count: \", test_data.count())\n",
    "\n",
    "# Apply standardization\n",
    "scaler = StandardScaler(inputCol=\"review_vector\", outputCol=\"review_scaled\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(train_data)\n",
    "scaled_data = scaler_model.transform(train_data)\n",
    "scaled_test = scaler_model.transform(test_data)\n",
    "\n",
    "# Drop original scalar column and rename scaled vector\n",
    "scaled_data = scaled_data.drop(\"Review Rating\", \"review_vector\").withColumnRenamed(\"review_scaled\", \"Review Rating (scaled)\")\n",
    "\n",
    "scaled_data = scaled_data.withColumn(\"Review Rating (scaled)\", vector_to_array(\"Review Rating (scaled)\")[0])\n",
    "scaled_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final datasets\n",
    "# test_data, train_data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from scipy.stats import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Drop non-informative columns\n",
    "train_data = train_data.drop(columns=[\"Customer ID\", \"Discount Applied\"])\n",
    "test_data = test_data.drop(columns=[\"Customer ID\", \"Discount Applied\"])\n",
    "\n",
    "\n",
    "\n",
    "# Identify categorical vs numerical targets\n",
    "categorical_cols = train_data.columns\n",
    "numerical_cols = train_data.columns['Review Rating (scaled)']\n",
    "\n",
    "# Set up parameter space for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 300),\n",
    "    'max_depth': randint(5, 30),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Train a model for each target column\n",
    "results = {}\n",
    "\n",
    "for target_col in train_data.columns:\n",
    "    feature_cols = train_data.columns[train_data.columns != target_col]\n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data[target_col]\n",
    "    X_test = test_data[feature_cols]\n",
    "    y_test = test_data[target_col]\n",
    "\n",
    "    # Choose model type\n",
    "    if target_col in numerical_cols:\n",
    "        base_model = RandomForestRegressor(random_state=42)\n",
    "        metric_name = \"MSE\"\n",
    "    else:\n",
    "        base_model = RandomForestClassifier(random_state=42)\n",
    "        metric_name = \"Accuracy\"\n",
    "\n",
    "    # Randomized hyperparameter tuning\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        cv=3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if target_col in numerical_cols:\n",
    "        score = mean_squared_error(y_test, y_pred)\n",
    "    else:\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        print(f\"\\nClassification Report for {target_col}:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    results[target_col] = f\"{metric_name}: {score:.4f}\"\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n=== Summary Results ===\")\n",
    "for col, score in results.items():\n",
    "    print(f\"{col}: {score}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
